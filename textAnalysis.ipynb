{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMY9l7tPqxEVhXF8fsrjVVj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PTDaISU8Ghrf"},"outputs":[],"source":["import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import textstat\n","import re\n","import os\n","\n","nltk.download('vader_lexicon')\n","nltk.download('punkt')\n","\n","input_df = pd.read_excel('Input.xlsx')\n","urls = input_df['URL']\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","def extract_text_from_url(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    title = soup.find('title').get_text()\n","\n","    paragraphs = soup.find_all('p')\n","    article_text = ' '.join([para.get_text() for para in paragraphs])\n","\n","    return title, article_text\n","\n","def analyze_text(text):\n","    sentences = nltk.sent_tokenize(text)\n","    num_sentences = len(sentences)\n","    words = nltk.word_tokenize(text)\n","    num_words = len(words)\n","    sentiment_scores = sid.polarity_scores(text)\n","    positive_score = sentiment_scores['pos']\n","    negative_score = sentiment_scores['neg']\n","    polarity_score = sentiment_scores['compound']\n","    subjectivity_score = textstat.text_standard(text, float_output=True)\n","    avg_sentence_length = num_words / num_sentences if num_sentences != 0 else 0\n","    complex_words = [word for word in words if textstat.syllable_count(word) > 2]\n","    percentage_complex_words = len(complex_words) / num_words * 100 if num_words != 0 else 0\n","    fog_index = textstat.gunning_fog(text)\n","    complex_word_count = len(complex_words)\n","    word_count = num_words\n","    syllables_per_word = textstat.syllable_count(text) / num_words if num_words != 0 else 0\n","    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I))\n","    avg_word_length = sum(len(word) for word in words) / num_words if num_words != 0 else 0\n","\n","    return {\n","        'positive_score': positive_score,\n","        'negative_score': negative_score,\n","        'polarity_score': polarity_score,\n","        'subjectivity_score': subjectivity_score,\n","        'avg_sentence_length': avg_sentence_length,\n","        'percentage_complex_words': percentage_complex_words,\n","        'fog_index': fog_index,\n","        'complex_word_count': complex_word_count,\n","        'word_count': word_count,\n","        'syllables_per_word': syllables_per_word,\n","        'personal_pronouns': personal_pronouns,\n","        'avg_word_length': avg_word_length\n","    }\n","\n","\n","for index, row in input_df.iterrows():\n","    url = row['URL']\n","    url_id = row['URL_ID']\n","    title, article_text = extract_text_from_url(url)\n","\n","\n","    with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n","        file.write(title + '\\n' + article_text)\n","\n","\n","output_df = pd.DataFrame()\n","\n","for index, row in input_df.iterrows():\n","    url_id = row['URL_ID']\n","    file_path = f'{url_id}.txt'\n","\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            text = file.read()\n","            analysis_results = analyze_text(text)\n","\n","\n","            row_data = row.to_dict()\n","            row_data.update(analysis_results)\n","            output_df = output_df.append(row_data, ignore_index=True)\n","\n","\n","output_df.to_excel('Output Data Structure.xlsx', index=False)\n","\n","print(\"Analysis complete. Results saved to 'Output Data Structure.xlsx'.\")\n"]}]}